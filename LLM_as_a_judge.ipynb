{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "I was inspired by the literature \"Juding LLM-as-a-Judge with MT-Bench and Chatbot Arena\". I would like to use this method to test the Chinese text generation of GPT-3.5 and text it with ERNIE-3.5"
      ],
      "metadata": {
        "id": "vlzdLFT8nJmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd_CDFTSRsMo",
        "outputId": "36b76336-c7a0-448e-860f-9295b7a07068"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.14.3-py3-none-any.whl (262 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.9/262.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.14.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the generated text from gpt-3.5 Turbo"
      ],
      "metadata": {
        "id": "Q_X_5FM8OEyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import http.client\n",
        "import json\n",
        "\n",
        "theme_list = [\"自由\",\"生命\",\"理想\",\"高山\",\"大海\"]\n",
        "from openai import OpenAI\n",
        "text_list = []\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key = \"sk-9qxqayMe8D42SRZ3r4yb4SRjdF0Xd2FwUmavZUJZXSTC6VnJ\",\n",
        "    base_url = \"https://api.chatanywhere.tech/v1\"\n",
        ")\n",
        "for theme in theme_list:\n",
        "  chat_completion = client.chat.completions.create(\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              f\"content\": \"请围绕{theme}这个主题写一首七言绝句,并且生成的内容只需要包括诗歌主体\",\n",
        "          }\n",
        "      ],\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "  )\n",
        "  text = chat_completion.choices[0].message.content\n",
        "  text_list.append(text)\n",
        "print(text_list)"
      ],
      "metadata": {
        "id": "21_-E4fpnih_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24fd8b1a-c01a-4bde-b5a0-b45e2d3d98f9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['乘风破浪展回帆，心中主题永不变。\\n奋力向前再出发，追寻梦想与希望。', '春风柔软抚青枝，\\n映入窗帘梦正醒。\\n花香扑鼻溢轻袖，\\n心灵荡漾一片情。', '诗歌主体：\\n追寻人生的辉煌道，\\n落英飞雁乐章悠。\\n探索自我在宇宙，\\n艰辛涤荡意识流。\\n\\n命运离奇如繁星，\\n争突映照心灵舟。\\n人生如戏剧性展，\\n创造璀璨人世炼。', '深秋的枫叶飘红落，\\n远山的云彩飞扬翻。\\n清心自在在山水，\\n诗意长存在人间。', '春风吹过万物苏，\\n花开芬芳色香浮。\\n阳光洒满湖泊畔，\\n映照彩云翩翩舞。']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using ERNIE-3.5 to evaluate the quality of the generated text"
      ],
      "metadata": {
        "id": "7zKBs5UbOQhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"vk05T4TAIxYy6TU6Pk0c9hMz\"\n",
        "secret_key = \"zGlgT0MScaGtvQ1lcUEGRLFEjgvGm08U\"\n",
        "import pandas\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def get_access_token():\n",
        "    \"\"\"\n",
        "    使用 API Key，Secret Key 获取access_token，替换下列示例中的应用API Key、应用Secret Key\n",
        "    \"\"\"\n",
        "\n",
        "    url = f\"https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id={api_key}&client_secret={secret_key}\"\n",
        "\n",
        "    payload = json.dumps(\"\")\n",
        "    headers = {\n",
        "        'Content-Type': 'application/json',\n",
        "        'Accept': 'application/json'\n",
        "    }\n",
        "\n",
        "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "    return response.json().get(\"access_token\")\n",
        "rate = {}\n",
        "for text in  text_list:\n",
        "  url = \"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/completions?access_token=\" + get_access_token()\n",
        "\n",
        "  payload = json.dumps({\n",
        "      \"messages\": [\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": f\"你的回答只需要一个数字。请你用以下标准：（评分等级为1到5）：①格式。生成的唐诗是否遵循写作规则,尤其注意对仗、句数、字数、平仄；②创新性。生成的唐诗是否抄袭现成的诗句，是否具有创新性；③相关性。生成的唐诗与给定主题是否相关；④美学。生成的唐诗是否符合中国诗词的审美，包括寓情于物等；⑤整体。从整体情况评价生成的唐诗。诗歌为:{text}\"\n",
        "          }\n",
        "      ],\n",
        "      \"response_format\":\"json_object\"\n",
        "  })\n",
        "  headers = {\n",
        "      'Content-Type': 'application/json'\n",
        "  }\n",
        "\n",
        "  response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "  print(response.text)"
      ],
      "metadata": {
        "id": "I0sFS2XXt4e-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c3999c6-7b86-4003-af97-a73791a05f11"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"id\":\"as-qckexqyjtf\",\"object\":\"chat.completion\",\"created\":1711804486,\"result\":\"{\\n    \\\"格式\\\": 3,\\n    \\\"创新性\\\": 3,\\n    \\\"相关性\\\": 4,\\n    \\\"美学\\\": 4,\\n    \\\"整体\\\": 4\\n}\",\"is_truncated\":false,\"need_clear_history\":false,\"finish_reason\":\"normal\",\"usage\":{\"prompt_tokens\":130,\"completion_tokens\":43,\"total_tokens\":173}}\n",
            "{\"id\":\"as-h4kstjyfha\",\"object\":\"chat.completion\",\"created\":1711804492,\"result\":\"{\\n    \\\"格式\\\": 3,\\n    \\\"创新性\\\": 3,\\n    \\\"相关性\\\": 3,\\n    \\\"美学\\\": 3,\\n    \\\"整体\\\": 3\\n}\",\"is_truncated\":false,\"need_clear_history\":false,\"finish_reason\":\"normal\",\"usage\":{\"prompt_tokens\":135,\"completion_tokens\":43,\"total_tokens\":178}}\n",
            "{\"id\":\"as-wi6t14jb0t\",\"object\":\"chat.completion\",\"created\":1711804496,\"result\":\"{\\n    \\\"格式\\\": 4,\\n    \\\"创新性\\\": 3,\\n    \\\"相关性\\\": 4,\\n    \\\"美学\\\": 4,\\n    \\\"整体\\\": 4\\n}\",\"is_truncated\":false,\"need_clear_history\":false,\"finish_reason\":\"normal\",\"usage\":{\"prompt_tokens\":169,\"completion_tokens\":43,\"total_tokens\":212}}\n",
            "{\"id\":\"as-t3sab5ymuc\",\"object\":\"chat.completion\",\"created\":1711804500,\"result\":\"{\\n    \\\"格式\\\": 4,\\n    \\\"创新性\\\": 3,\\n    \\\"相关性\\\": 4,\\n    \\\"美学\\\": 4,\\n    \\\"整体\\\": 4\\n}\",\"is_truncated\":false,\"need_clear_history\":false,\"finish_reason\":\"normal\",\"usage\":{\"prompt_tokens\":136,\"completion_tokens\":43,\"total_tokens\":179}}\n",
            "{\"id\":\"as-rfuzmpk4xa\",\"object\":\"chat.completion\",\"created\":1711804504,\"result\":\"{\\n    \\\"格式\\\": 4,\\n    \\\"创新性\\\": 3,\\n    \\\"相关性\\\": 4,\\n    \\\"美学\\\": 4,\\n    \\\"整体\\\": 4\\n}\",\"is_truncated\":false,\"need_clear_history\":false,\"finish_reason\":\"normal\",\"usage\":{\"prompt_tokens\":135,\"completion_tokens\":43,\"total_tokens\":178}}\n"
          ]
        }
      ]
    }
  ]
}